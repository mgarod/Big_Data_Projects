IMPLEMENTATION:

Required Libraries: pyspark, boto (for Amazon S3 connection)

The python script, similarity.py, is implemented in a procedural way, without loops and only using RDD methods.

No large data structures were used, beyond the standard python implementations of lists and tuples.

Even when run locally, the data for the script is pulled from our Amazon S3 bucket.

IMPROVEMENTS:

We could improve the spark algorithm in the similarity.py script by eliminating loops from the entirely. We do not loop over entire collections, which would violate the ideas of the distributed algorithm, but we do create mapping functions which contain loops therein. It may be beneficial to eliminate these loops and rethink the functions without them.

The speed of the algorithm could be improved by more caching. Currently we only cache the TF-IDF matrix, because it is used several more times throughout the script. If space is not that important, more intermediate RDDs could be cached.

On the way to our final result, we generate many intermediate RDDs. With some more time and thorough testing, we could put more RDD methods in a single step, which may improve speed of the script.

We feel highly skilled in our MapReduce algorithm design. Given enough time, we believe that we could have developed the document-document similarity ranking as well.

An improvement to make our code run faster on large amounts of data with multiple Amazon Web Services nodes, would be to cache the data in the node's ram. This will increase the speed when calling upon distributed data.
